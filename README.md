# zhy-class
第三次的考核任务
将numpy导入为np                
从matplotlib导入颜色     
从sklearn导入svm            
从sklearn.svm导入SVC
从sklearn导入model_selection
导入matplotlib.pyplot作为plt
导入matplotlib为mpl
这些是约会的包，sklearn里面的包主要用来建立SVM模型的，matplotlib主要是用来绘制散点图
从sklearn.datasets导入load_iris
iris = load_iris（）
x = iris.data
y = iris.target
用了sklearn里面的数据集来鸢尾花的数据集用了x和y分别存放了鸢尾花的特征和标签。打印出来看下：
打印（iris.target）
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
打印（iris.data）
[[5.1 3.5 1.4 0.2]
 [4.9 3. 1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5. 3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5. 3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5.4 3.7 1.5 0.2]
 [4.8 3.4 1.6 0.2]
 [4.8 3. 1.4 0.1]
 [4.3 3. 1.1 0.1]
 [5.8 4. 1.2 0.2]
 [5.7 4.4 1.5 0.4]
 [5.4 3.9 1.3 0.4]
 [5.1 3.5 1.4 0.3]
 [5.7 3.8 1.7 0.3]
 [5.1 3.8 1.5 0.3]
 [5.4 3.4 1.7 0.2]
 [5.1 3.7 1.5 0.4]
 [4.6 3.6 1. 0.2]
 [5.1 3.3 1.7 0.5]
 [4.8 3.4 1.9 0.2]
 [5. 3. 1.6 0.2]
 [5. 3.4 1.6 0.4]
 [5.2 3.5 1.5 0.2]
 [5.2 3.4 1.4 0.2]
 [4.7 3.2 1.6 0.2]
 [4.8 3.1 1.6 0.2]
 [5.4 3.4 1.5 0.4]
 [5.2 4.1 1.5 0.1]
 [5.5 4.2 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5. 3.2 1.2 0.2]
 [5.5 3.5 1.3 0.2]
 [4.9 3.1 1.5 0.1]
 [4.4 3. 1.3 0.2]
 [5.1 3.4 1.5 0.2]
 [5. 3.5 1.3 0.3]
 [4.5 2.3 1.3 0.3]
 [4.4 3.2 1.3 0.2]
 [5. 3.5 1.6 0.6]
 [5.1 3.8 1.9 0.4]
 [4.8 3. 1.4 0.3]
 [5.1 3.8 1.6 0.2]
 [4.6 3.2 1.4 0.2]
 [5.3 3.7 1.5 0.2]
 [5. 3.3 1.4 0.2]
 [7. 3.2 4.7 1.4]
 [6.4 3.2 4.5 1.5]
 [6.9 3.1 4.9 1.5]
 [5.5 2.3 4. 1.3]
 [6.5 2.8 4.6 1.5]
 [5.7 2.8 4.5 1.3]
 [6.3 3.3 4.7 1.6]
 [4.9 2.4 3.3 1.]
 [6.6 2.9 4.6 1.3]
 [5.2 2.7 3.9 1.4]
 [5. 2. 3.5 1.]
 [5.9 3. 4.2 1.5]
 [6. 2.2 4. 1.]
 [6.1 2.9 4.7 1.4]
 [5.6 2.9 3.6 1.3]
 [6.7 3.1 4.4 1.4]
 [5.6 3. 4.5 1.5]
 [5.8 2.7 4.1 1.]
 [6.2 2.2 4.5 1.5]
 [5.6 2.5 3.9 1.1]
 [5.9 3.2 4.8 1.8]
 [6.1 2.8 4. 1.3]
 [6.3 2.5 4.9 1.5]
 [6.1 2.8 4.7 1.2]
 [6.4 2.9 4.3 1.3]
 [6.6 3. 4.4 1.4]
 [6.8 2.8 4.8 1.4]
 [6.7 3. 5. 1.7]
 [6. 2.9 4.5 1.5]
 [5.7 2.6 3.5 1.]
 [5.5 2.4 3.8 1.1]
 [5.5 2.4 3.7 1.]
 [5.8 2.7 3.9 1.2]
 [6. 2.7 5.1 1.6]
 [5.4 3. 4.5 1.5]
 [6. 3.4 4.5 1.6]
 [6.7 3.1 4.7 1.5]
 [6.3 2.3 4.4 1.3]
 [5.6 3. 4.1 1.3]
 [5.5 2.5 4. 1.3]
 [5.5 2.6 4.4 1.2]
 [6.1 3. 4.6 1.4]
 [5.8 2.6 4. 1.2]
 [5. 2.3 3.3 1.]
 [5.6 2.7 4.2 1.3]
 [5.7 3. 4.2 1.2]
 [5.7 2.9 4.2 1.3]
 [6.2 2.9 4.3 1.3]
 [5.1 2.5 3. 1.1]
 [5.7 2.8 4.1 1.3]
 [6.3 3.3 6. 2.5]
 [5.8 2.7 5.1 1.9]
 [7.1 3. 5.9 2.1]
 [6.3 2.9 5.6 1.8]
 [6.5 3. 5.8 2.2]
 [7.6 3. 6.6 2.1]
 [4.9 2.5 4.5 1.7]
 [7.3 2.9 6.3 1.8]
 [6.7 2.5 5.8 1.8]
 [7.2 3.6 6.1 2.5]
 [6.5 3.2 5.1 2.]
 [6.4 2.7 5.3 1.9]
 [6.8 3. 5.5 2.1]
 [5.7 2.5 5. 2.]
 [5.8 2.8 5.1 2.4]
 [6.4 3.2 5.3 2.3]
 [6.5 3. 5.5 1.8]
 [7.7 3.8 6.7 2.2]
 [7.7 2.6 6.9 2.3]
 [6. 2.2 5. 1.5]
 [6.9 3.2 5.7 2.3]
 [5.6 2.8 4.9 2.]
 [7.7 2.8 6.7 2.]
 [6.3 2.7 4.9 1.8]
 [6.7 3.3 5.7 2.1]
 [7.2 3.2 6. 1.8]
 [6.2 2.8 4.8 1.8]
 [6.1 3. 4.9 1.8]
 [6.4 2.8 5.6 2.1]
 [7.2 3. 5.8 1.6]
 [7.4 2.8 6.1 1.9]
 [7.9 3.8 6.4 2.]
 [6.4 2.8 5.6 2.2]
 [6.3 2.8 5.1 1.5]
 [6.1 2.6 5.6 1.4]
 [7.7 3. 6.1 2.3]
 [6.3 3.4 5.6 2.4]
 [6.4 3.1 5.5 1.8]
 [6. 3. 4.8 1.8]
 [6.9 3.1 5.4 2.1]
 [6.7 3.1 5.6 2.4]
 [6.9 3.1 5.1 2.3]
 [5.8 2.7 5.1 1.9]
 [6.8 3.2 5.9 2.3]
 [6.7 3.3 5.7 2.5]
 [6.7 3. 5.2 2.3]
 [6.3 2.5 5. 1.9]
 [6.5 3. 5.2 2.]
 [6.2 3.4 5.4 2.3]
 [5.9 3. 5.1 1.8]]
x_train，x_test，y_train，y_test = model_selection.train_test_split（x，y，random_state = 1，test_size = 0.3）
这一步用了sklearn中的model_selection中split方法将继续的x，y数据划分为训练集和测试集，划分的比例按照7：3，7是训练集，3是测试集，多用点训练集训练的效果更好，但是测试集也不能太少，防止出现偶然性。random_state是使用随机选取的，下面的模型训练没有将提供的150个数据全部使用，让这个random_state = 1就是防止多次测试替换的测试集和训练集的长度不一样。
def classifier（）：
    clf = svm.SVC（C = 2，内核='rbf'，decision_function_shape ='ovr'）
    返回CLF
定义了一个模型训练函数，采用了高斯核函数，对鸢尾花的分类来说高斯核函数比线性核函数的精准度要高一点，decision_function_shape的训练用了ovr一对多。这题就是将3个面进行了3次操作，一种花作为一个整体，另外两种花作为一个整体进行划分，划出这个面。
clf = classifier（）
调用模型训练函数，训练了一个svm模型用clf来表示这个模型。
def火车（clf，x_train，y_train）：
    clf.fit（x_train，y_train.ravel（））
训练鸢尾花测试集的函数，里面用了新建立的svm模型，用了fit方法进行模型训练，.ravel的意思是把数组变成1维。
火车（clf，x_train，y_train）
称为了这个模型训练函数，现在的测试集已经完全训练完成了。
def show_accuracy（a，b，tip）：
    acc = a.ravel（）== b.ravel（）
    打印（'％s精度：％。3f'％（tip，np.mean（acc）））
这个是模型预测的函数，acc显示了预测值和实际标签的重复的个数，然后用了mean函数计算了这个比例
def print_accuracy（clf，x_train，y_train，x_test，y_test）：
    打印（'trianing预测：％。3f'％（clf.score（x_train，y_train）））
    print（'测试预测：％。3f'％（clf.score（x_test，y_test）））
    show_accuracy（clf.predict（x_train），y_train，'训练数据'）
    show_accuracy（clf.predict（x_test），y_test，“测试数据”）
    print（'decision_function：\ n'，clf.decision_function（x_train））
clf.score用了svm模型来进行测试，会测试出来样本和实际输出值之间的相对，这里面分别对训练集和测试集用了这个方法，后面又调用了之前定义的模型预测的函数，用clf.predict来进行预测然后和标签一起用这个show_accuracy函数进行输出。输出的值：
预测数：0.981
测试预测：1.000
训练数据准确度：0.981
测试数据精度：1.000
训练的效果很好好我觉得有3个方面，1是我用了4个特征进行模型的训练，相对准确度肯定高一点，2是我精选了高斯核函数，模型的训练效果比较好，3是我的惩罚参数c定义的比较大，惩罚的比较大，区别的更加明显，所以测试的时候比较准确。
print（'decision_function：\ n'，clf.decision_function（x_train））
这个函数用输出了训练集到分割面的距离，分别三个单独的训练集合到分割面的距离，正的代表在单独分割面的那边，负的代表在分割面的两个的那边输出的数据
Decision_function：
 [[-0.18196725 0.85337686 2.3285904]
 [2.30717586 0.90682637 -0.21400223]
 [2.35539818 0.83973745 -0.19513563]
 [2.34506236 0.8508829 -0.19594527]
 [-0.29839836 2.33764794 0.96075043]
 [2.34996149 0.85576405 -0.20572554]
 [2.3414933 0.85466882 -0.19616213]
 [-0.18766553 0.87216807 2.31549745]
 [-0.26887171 0.9295235 2.33934821]
 [-0.23030324 0.84159652 2.38870672]
 [-0.26387698 0.80535666 2.45852033]
 [-0.31086747 1.14925417 2.1616133]
 [-0.28495531 2.28041859 1.00453672]
 [-0.19940761 0.8517881 2.34761951]
 [-0.29737793 2.42310286 0.87427507]
 [2.33941489 0.85785298 -0.19726788]
 [-0.29815029 0.96784471 2.33030558]
 [-0.30593244 1.12229023 2.18364221]
 [2.34685584 0.84443973 -0.19129557]
 [2.35344161 0.83255587 -0.18599747]
 [-0.25553885 0.77470343 2.48083543]
 [2.34268004 0.83981125 -0.18249129]
 [-0.30817311 1.10372995 2.20444316]
 [-0.2847705 1.01265099 2.27211951]
 [-0.27059816 2.41840955 0.8521886]
 [-0.32205442 2.33319503 0.9888594]
 [-0.23890442 0.7843532 2.45455122]
 [-0.31103766 1.12884842 2.18218924]
 [2.34060181 0.85948043 -0.20008224]
 [-0.27197375 2.48289358 0.78908017]
 [-0.28653016 2.46075873 0.82577143]
 [-0.28230783 0.93123581 2.35107202]
 [-0.19177149 2.32170922 0.87006227]
 [-0.21949874 0.71949874 2.5]
 [-0.30991308 2.39693632 0.91297676]
 [2.34216722 0.86285484 -0.20502206]
 [2.34407581 0.85404312 -0.19811893]
 [2.33248133 0.85140273 -0.18388406]
 [-0.25985208 0.83922768 2.4206244]
 [2.33456436 0.83704915 -0.17161351]
 [-0.30810633 1.07885245 2.22925387]
 [-0.28846895 0.96023753 2.32823142]
 [-0.30207285 1.01137316 2.29069969]
 [2.34899367 0.857538 -0.20653167]
 [2.31493965 0.90316779 -0.21810743]
 [-0.2901343 2.48478792 0.80534639]
 [2.34407581 0.85404312 -0.19811893]
 [-0.23409929 0.86488389 2.3692154]
 [-0.29705798 2.44779516 0.84926281]
 [-0.21073432 0.83643439 2.37429993]
 [-0.26687842 0.92102106 2.34585736]
 [-0.29085793 2.40656361 0.88429432]
 [-0.25286033 0.83538926 2.41747106]
 [-0.21641694 0.81381629 2.40260065]
 [-0.24268298 2.43513104 0.80755195]
 [2.30004121 0.88012468 -0.1801659]
 [-0.28472651 2.47783303 0.80689348]
 [2.30000868 0.86583875 -0.16584743]
 [-0.28125321 2.43626143 0.84499179]
 [-0.27711822 2.24865615 1.02846207]
 [2.33667963 0.87323139 -0.20991102]
 [-0.28189565 2.30696732 0.97492833]
 [2.32482702 0.88233804 -0.20716506]
 [2.30004621 0.90146849 -0.20151469]
 [-0.24088254 0.78453341 2.45634913]
 [-0.30374814 0.99027018 2.31347795]
 [-0.24624474 0.78138426 2.46486048]
 [2.35549299 0.84448124 -0.19997422]
 [2.34546562 0.85797239 -0.20343801]
 [-0.30168023 1.11828704 2.1833932]
 [2.33698234 0.85074293 -0.18772528]
 [-0.30167408 0.97095861 2.33071546]
 [2.33365584 0.86645914 -0.20011499]
 [-0.28793328 0.97654919 2.31138409]
 [-0.31080577 1.10218108 2.20862469]
 [2.34498032 0.83936436 -0.18434467]
 [-0.26579995 0.86775706 2.39804289]
 [2.32721592 0.82947788 -0.1566938]
 [-0.29602304 2.40391015 0.89211289]
 [2.32777768 0.84654544 -0.17432313]
 [-0.26171322 2.44515165 0.81656157]
 [-0.22705134 2.31820292 0.90884842]
 [2.35618737 0.84386934 -0.20005671]
 [2.3472258 0.85005761 -0.19728341]
 [-0.18600303 2.30616804 0.87983499]
 [2.33471351 0.82395333 -0.15866684]
 [-0.3025824 2.38554523 0.91703717]
 [-0.31406907 2.29739323 1.01667584]
 [2.35080629 0.85206879 -0.20287508]
 [-0.2847062 2.31023729 0.9744689]
 [-0.30796873 2.40573645 0.90223228]
 [-0.29635043 2.31709966 0.97925077]
 [-0.26118542 2.33216203 0.92902339]
 [-0.30167408 0.97095861 2.33071546]
 [2.31971244 0.898664 -0.21837645]
 [2.32860842 0.87819956 -0.20680798]
 [-0.268245 0.96825973 2.29998527]
 [-0.28329534 2.4877447 0.79555064]
 [-0.25212285 0.95068468 2.30143817]
 [-0.22135372 2.47077226 0.75058147]
 [-0.30304275 1.13908314 2.16395961]
 [-0.28125799 0.92368468 2.35757331]
 [-0.3032082 2.21034951 1.09285869]
 [-0.25220229 0.8058622 2.44634009]
 [2.34407581 0.85404312 -0.19811893]
因为用了random_state = 1是随机选出来的数，没有使用整个样本。所以数据不是很多。
iris_feature ='分隔长度'，'分隔宽度'，'花瓣长度'，'花瓣宽度'＃提取特征
＃'花瓣片长度''花瓣片宽度''花瓣长度''花瓣宽度'
提取iris的特征表达方式
用matplotlib画图，画的是散点图所以就只用两个特征。
x1_min，x1_max = x [：，0] .min（），x [：，0] .max（）
x2_min，x2_max = x [：，1] .min（），x [：，1] .max（）
提取第一列和第二列，就是代表花瓣片长度，和花瓣片宽度的两列。
x1，x2 = np.mgrid [x1_min：x1_max：200j，x2_min：x2_max：200j]
定义了x1和x2两个副本里面存的是花瓣片长度和花瓣片宽度最大间隔之间的间隔为200j的所有点，这些点是为了给后面画图的时候图片上色准备的。
grid_test = np.stack（（x1.flat，x2.flat），axis = 1）
这个将x1，x2添加变成二维的就是为了给图片的底色分段3种颜色这个要配合下面这个
z = clf.decision_function（grid_test）
这个是为了用svm模型计算出来的那些细小的点分别到分割面的距离。
将grid_test打印出来看下肯定是细小的坐标
gril_test：
 [[4.3 2.]
 [4.3 2.0120603]
 [4.3 2.0241206]
 ...， 
 [7.9 4.3758794]
 [7.9 4.3879397]
 [7.9 4.4]]
将预测的分别的距离打印出来肯定也是3列，代表3种花分别与分割面的关系
 print（'到决策平面的距离：\ n'，z）
距决策平面的距离：
 [[2.08287197 1.01262753 -0.0954995]
 [2.08496422 1.01165707 -0.09662129]
 [2.08709407 1.01066221 -0.09775628]
 ...， 
 [-0.07366204 0.93929852 2.13436351]
 [-0.07259876 0.9395262 2.13307256]
 [-0.07153888 0.93975728 2.13178161]]
grid_hat = clf.predict（grid_test） 
这个用了建立的svm模型用样本来预测，而这个样本是无数个点，预测可以将他们合并3种点
 打印（'grid_hat：\ n'，grid_hat）
打印出来看下
[0 0 0 ...，2 2 2]
grid_hat = grid_hat.reshape（x1.shape）
将预测出来的变成x1那种二维的形式可以打印出来
cm_light = mpl.colors.ListedColormap（['＃A0FFA0'，'＃FFA0A0'，'＃A0A0FF']）
cm_dark = mpl.colors.ListedColormap（['g'，'b'，'r']）
分别给后面的图片上色，前面的是浅色，后面的是3个深色。
plt.pcolormesh（x1，x2，grid_hat，cmap = cm_light） 
呈现图片的底色
plt.pcolormesh（x1，x2，grid_hat，cmap = cm_light）   
用三种浅的颜色随机给图片画上底色，不同的颜色所在的区域代表不同的花
plt.scatter（x [:, 0]，x [:, 1]，c = y，edgecolor ='k'，s = 50，cmap = cm_dark） 
将抽取出来的x的点画在图上
plt.scatter（x_test [:, 0]，x_test [:, 1]，s = 120，facecolor ='none'，zorder = 10）   
将测试集的点重新画在图上，测试之前画的图是否正确
plt.xlabel（iris_feature [0]，fontsize = 13）
    plt.ylabel（iris_feature [1]，fontsize = 13）
    plt.xlim（x1_min，x1_max）
    plt.ylim（x2_min，x2_max）
    plt.title（'虹膜数据分类中的'svm'，fontsize = 15）
    plt.grid（）
    plt.show（）
这些步骤就是改变标题大小和进行图型替换
x = x [：，0：2]
x_train，x_test，y_train，y_test = model_selection.train_test_split（x，y，random_state = 1，test_size = 0.3）
clf = classifier（）
火车（clf，x_train，y_train）
画（clf，x）
取出了前两个特征然后进行这个画图方法
